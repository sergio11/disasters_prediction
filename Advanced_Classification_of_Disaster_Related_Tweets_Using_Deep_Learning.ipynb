{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Classification of Disaster-Related Tweets Using Deep Learning\n",
        "\n",
        "## Introduction\n",
        "In this project, we will build a deep learning model using Keras to classify tweets as real or fake in the context of disasters. This task is inspired by the \"NLP with Disaster Tweets\" challenge and enriched with additional data to improve model performance and insights. The dataset provides a fascinating opportunity to explore Natural Language Processing (NLP) techniques on real-world data.\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset Overview\n",
        "### Context\n",
        "The dataset contains over 11,000 tweets associated with disaster-related keywords such as \"crash,\" \"quarantine,\" and \"bush fires.\" The data structure is based on the original \"Disasters on social media\" dataset. It includes:\n",
        "- **Tweets:** The text of the tweet.\n",
        "- **Keywords:** Specific disaster-related keywords.\n",
        "- **Location:** The geographical information provided in the tweets.\n",
        "\n",
        "These tweets were collected on **January 14th, 2020** and cover major events including:\n",
        "- The eruption of Taal Volcano in Batangas, Philippines.\n",
        "- The emerging outbreak of **Coronavirus (COVID-19)**.\n",
        "- The devastating **Bushfires in Australia**.\n",
        "- The **Iranian downing of flight PS752**.\n",
        "\n",
        "### Important Note\n",
        "The dataset contains text that may include profane, vulgar, or offensive language. Please approach with caution during analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## Project Goals\n",
        "### Inspiration\n",
        "The primary goal of this project is to develop a machine learning model capable of identifying whether a tweet is genuinely related to a disaster or not. This involves:\n",
        "1. Enriching the already available data with newly collected, manually classified tweets.\n",
        "2. Leveraging state-of-the-art deep learning methods to extract meaningful insights.\n",
        "3. Applying NLP techniques to preprocess, clean, and tokenize the tweets for model training.\n",
        "\n",
        "This notebook will walk through the process of preparing the dataset, building a deep learning model, and evaluating its performance. By the end, we aim to achieve a robust model that can classify disaster tweets with high accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## Why It Matters\n",
        "Effective classification of disaster-related tweets has numerous practical applications:\n",
        "- **Emergency Response:** Helps organizations identify critical information in real time.\n",
        "- **Resource Allocation:** Facilitates better planning by focusing on real disasters.\n",
        "- **Misinformation Control:** Mitigates the spread of false information during crises."
      ],
      "metadata": {
        "id": "9XshkOLmRDf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "HcOv7JePRGWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "qYHdNYEyRvS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('tweets.csv')\n",
        "\n",
        "# Display the first few rows to inspect the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Display dataset information (columns, data types, non-null counts)\n",
        "print(data.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpLaQCsQTbg1",
        "outputId": "cecf39ee-93ca-4e7c-83ab-7fd2128ce6f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id keyword        location  \\\n",
            "0   0  ablaze             NaN   \n",
            "1   1  ablaze             NaN   \n",
            "2   2  ablaze   New York City   \n",
            "3   3  ablaze  Morgantown, WV   \n",
            "4   4  ablaze             NaN   \n",
            "\n",
            "                                                text  target  \n",
            "0  Communal violence in Bhainsa, Telangana. \"Ston...       1  \n",
            "1  Telangana: Section 144 has been imposed in Bha...       1  \n",
            "2  Arsonist sets cars ablaze at dealership https:...       1  \n",
            "3  Arsonist sets cars ablaze at dealership https:...       1  \n",
            "4  \"Lord Jesus, your love brings freedom and pard...       0  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11370 entries, 0 to 11369\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        11370 non-null  int64 \n",
            " 1   keyword   11370 non-null  object\n",
            " 2   location  7952 non-null   object\n",
            " 3   text      11370 non-null  object\n",
            " 4   target    11370 non-null  int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 444.3+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure the dataset contains the required columns, such as:\n",
        "- `text`: The tweet content.\n",
        "- `label`: The classification label indicating whether the tweet is fake or not."
      ],
      "metadata": {
        "id": "dcPeZf5FTjFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify required columns\n",
        "assert 'text' in data.columns, \"Column 'text' is missing in the dataset.\"\n",
        "assert 'target' in data.columns, \"Column 'target' is missing in the dataset.\""
      ],
      "metadata": {
        "id": "yqjhyGBwThDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will split the dataset into training and validation sets using an 80%-20% ratio."
      ],
      "metadata": {
        "id": "7_fg7YBETl4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Features (tweet content) and labels (fake/true)\n",
        "X = data['text']       # Features\n",
        "y = data['target']      # Labels\n",
        "\n",
        "# Split the dataset (80% training, 20% validation)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "GeGrzSYBTnj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the training and validation datasets as separate CSV files for later use."
      ],
      "metadata": {
        "id": "8LNG0zrzTuHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine features and labels into dataframes\n",
        "train_df = pd.DataFrame({'text': X_train, 'label': y_train})\n",
        "test_df = pd.DataFrame({'text': X_test, 'label': y_test})\n",
        "\n",
        "# Save the dataframes to CSV files\n",
        "train_df.to_csv('train.csv', index=False)\n",
        "test_df.to_csv('test.csv', index=False)\n",
        "\n",
        "print(\"Datasets have been saved successfully:\")\n",
        "print(\"- Training set: train.csv\")\n",
        "print(\"- Validation set: test.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMnWSea3Tvv1",
        "outputId": "cd62abf0-9785-4674-eacc-0e39c3ebb811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets have been saved successfully:\n",
            "- Training set: train.csv\n",
            "- Validation set: test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Visualization\n",
        "\n",
        "In this section, we conduct a detailed exploratory data analysis (EDA) to understand the structure and distribution of our dataset. EDA is crucial for identifying potential challenges, trends, and biases within the data, which in turn helps in selecting the most suitable models and preprocessing steps.\n",
        "\n",
        "### 2.1 Dataset Size\n",
        "\n",
        "First, we print the size of both the training and testing datasets. This gives us an idea of how many data points we are working with, which is essential when evaluating model performance and understanding the balance of the dataset.\n"
      ],
      "metadata": {
        "id": "jKCAkGGcLs3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training dataset size: \", len(X_train))\n",
        "print(\"Testing dataset size: \", len(X_test))"
      ],
      "metadata": {
        "id": "UmgMh_5rMzU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Class Distribution in Training Data\n",
        "Next, we examine the distribution of the target variable in the training set. The target variable indicates whether a tweet is related to a **disaster (1) or not (0)**. Understanding the class distribution helps identify if the dataset is imbalanced, which could influence the choice of model or evaluation metrics (e.g., using precision-recall curves instead of accuracy)."
      ],
      "metadata": {
        "id": "KD4GX_WXM7N-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the count of tweets for each target class in the training set\n",
        "X_train['target'].value_counts()"
      ],
      "metadata": {
        "id": "66gGWmGzNmBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then plot a histogram of the target variable to visualize the distribution of disaster and non-disaster tweets in the training data. This graphical representation makes it easier to spot any imbalance."
      ],
      "metadata": {
        "id": "f52ANSCjNqGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train['target'].hist()\n",
        "plt.ylabel(\"# tweets\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-IhHDu7xNtWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Exploratory Analysis of Tweet Length\n",
        "### 2.3.1 Word Count per Tweet\n",
        "A useful aspect of text data is the length of the text. Here, we explore the number of words per tweet. This metric can help us understand the average tweet size for both disaster-related and non-disaster-related tweets, which may inform decisions on feature engineering, such as tokenization or padding.\n",
        "\n",
        "We use histograms to display the word count for each category of tweets (disaster vs. non-disaster)."
      ],
      "metadata": {
        "id": "enq8QFrDNxd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of words per tweet for each category (disaster vs non-disaster)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
        "\n",
        "tweet_len_0 = X_train[X_train['target'] == 0]['text'].str.split().map(lambda x: len(x))  # Non-disaster tweets\n",
        "tweet_len_1 = X_train[X_train['target'] == 1]['text'].str.split().map(lambda x: len(x))  # Disaster tweets\n",
        "\n",
        "ax1.hist(tweet_len_0, color='green')\n",
        "ax1.set_title('Non-disaster tweets')\n",
        "\n",
        "ax2.hist(tweet_len_1, color='red')\n",
        "ax2.set_title('Disaster tweets')\n",
        "\n",
        "fig.suptitle('Word Count per Tweet')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yIBnAGqyN9hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3.2 Unique Word Count per Tweet\n",
        "Next, we analyze the number of unique words per tweet. This measure indicates how diverse the vocabulary is for each tweet and can be an important factor when building feature sets **like word embeddings or TF-IDF.**"
      ],
      "metadata": {
        "id": "1dLpfGHaOI6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of unique words per tweet for each category\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
        "\n",
        "tweet_len_0 = X_train[X_train['target'] == 0]['text'].str.split().map(lambda x: len(set(x)))  # Non-disaster tweets\n",
        "tweet_len_1 = X_train[X_train['target'] == 1]['text'].str.split().map(lambda x: len(set(x)))  # Disaster tweets\n",
        "\n",
        "ax1.hist(tweet_len_0, color='green')\n",
        "ax1.set_title('Non-disaster tweets')\n",
        "\n",
        "ax2.hist(tweet_len_1, color='red')\n",
        "ax2.set_title('Disaster tweets')\n",
        "\n",
        "fig.suptitle('Unique Word Count per Tweet')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kP3fAQrLOkdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.3 Average Word Length per Tweet\n",
        "Lastly, we investigate the average length of the words used in the tweets. This measure provides insight into the complexity or simplicity of the language used in disaster vs. non-disaster tweets. A higher average word length might indicate more formal or technical language, whereas shorter words could suggest more informal communication."
      ],
      "metadata": {
        "id": "nOPwjjZnObNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average word length per tweet for each category\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
        "\n",
        "tweet_len_0 = X_train[X_train['target'] == 0]['text'].str.split().map(lambda x: np.mean([len(i) for i in x]))  # Non-disaster tweets\n",
        "tweet_len_1 = X_train[X_train['target'] == 1]['text'].str.split().map(lambda x: np.mean([len(i) for i in x]))  # Disaster tweets\n",
        "\n",
        "ax1.hist(tweet_len_0, color='green')\n",
        "ax1.set_title('Non-disaster tweets')\n",
        "\n",
        "ax2.hist(tweet_len_1, color='red')\n",
        "ax2.set_title('Disaster tweets')\n",
        "\n",
        "fig.suptitle('Average Word Length per Tweet')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uoO_vlmmOfJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Further Feature Calculations\n",
        "\n",
        "In addition to the basic tweet length analysis, we could calculate several other features that may provide additional insights for modeling. These features include:\n",
        "\n",
        "* Number of **words** at the end of a tweet\n",
        "* Number of **URLs** per tweet\n",
        "* **Average** number of characters per tweet\n",
        "* Number of **characters** per tweet\n",
        "* Number of punctuation marks per tweet\n",
        "* Number of **hashtags** per tweet\n",
        "* Number of **mentions** (@) per tweet\n",
        "\n",
        "These additional features could be crucial when constructing advanced models or for improving the understanding of tweet content."
      ],
      "metadata": {
        "id": "nZbot-lqMuW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Stopwords Analysis\n",
        "\n",
        "Stopwords are words that do not carry significant meaning by themselves, but help structure or modify other words in a sentence. These include articles, pronouns, prepositions, adverbs, and some verbs. In natural language processing (NLP), stopwords are typically removed because they do not add value to the analysis. For example, search engines like Google do not consider stopwords when indexing content, but they are used when displaying results.\n",
        "\n",
        "To explore which stopwords are most common in the dataset, we can use the following approach:\n",
        "\n"
      ],
      "metadata": {
        "id": "OMf3sLL0P4B9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords.words('english')\n",
        "\n",
        "def plot_stopwords(label):\n",
        "    tweets_stopwords = {}\n",
        "    for words in X_train[X_train['target'] == label]['text'].str.split():\n",
        "        sw = list(set(words).intersection(stopwords.words('english')))\n",
        "        for w in sw:\n",
        "            if w in tweets_stopwords.keys():\n",
        "                tweets_stopwords[w] += 1\n",
        "            else:\n",
        "                tweets_stopwords[w] = 1\n",
        "\n",
        "    top = sorted(tweets_stopwords.items(), key=lambda x:x[1],reverse=True)[:10]\n",
        "    plt.bar(*zip(*top))\n",
        "    plt.show()\n",
        "\n",
        "plot_stopwords(0)\n",
        "plot_stopwords(1)"
      ],
      "metadata": {
        "id": "qSu_lhXcQKqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will display the 10 most frequent stopwords in disaster and non-disaster tweets separately."
      ],
      "metadata": {
        "id": "vU1BvrsbQNDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Punctuation Marks Analysis\n",
        "\n",
        "Next, we analyze the punctuation marks used in the tweets. Punctuation marks can play a role in sentiment analysis and text classification. We examine the most frequently used punctuation marks in both disaster-related and non-disaster-related tweets:"
      ],
      "metadata": {
        "id": "bcYwaomVQ_dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def plot_punctuation(label):\n",
        "    tweets_punctuation = {}\n",
        "    for words in X_train[X_train['target'] == label]['text'].str.split():\n",
        "        sw = list(set(words).intersection(string.punctuation))\n",
        "        for w in sw:\n",
        "            if w in tweets_punctuation.keys():\n",
        "                tweets_punctuation[w] += 1\n",
        "            else:\n",
        "                tweets_punctuation[w] = 1\n",
        "\n",
        "    top = sorted(tweets_punctuation.items(), key=lambda x:x[1], reverse=True)[:20]\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(*zip(*top))\n",
        "    plt.show()\n",
        "\n",
        "plot_punctuation(0)\n",
        "plot_punctuation(1)\n"
      ],
      "metadata": {
        "id": "peCtQivpRGC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will plot the most common punctuation marks for both types of tweets."
      ],
      "metadata": {
        "id": "cu06gHWIRKAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 N-grams Analysis\n",
        "\n",
        "Finally, we perform an analysis of n-grams, which are sequences of n consecutive words. N-grams are useful for capturing patterns in text and can help us understand frequently occurring phrases or expressions."
      ],
      "metadata": {
        "id": "J6cTTlJQRMvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(ngram_range=(2, 2))\n",
        "sum_words = cv.fit_transform(X_train['text']).sum(axis=0)\n",
        "\n",
        "# Calculate frequency of n-grams\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)[:20]\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.barh(*zip(*words_freq))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7fXycAI9RRYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This analysis will highlight the top 20 most frequent 2-grams (bigrams) in the dataset."
      ],
      "metadata": {
        "id": "exmLrA7LRdjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These visualizations and additional feature calculations provide deeper insights into the structure and content of the dataset. By understanding features like word count, stopwords, punctuation marks, and n-grams, we can better prepare our data for modeling. Moreover, the analysis helps identify any potential biases or patterns that could influence the performance of the model, ensuring that we approach the task of disaster tweet classification in a well-informed manner."
      ],
      "metadata": {
        "id": "2X5j6EoGRflu"
      }
    }
  ]
}