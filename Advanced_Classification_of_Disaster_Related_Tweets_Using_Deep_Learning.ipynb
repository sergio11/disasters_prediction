{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Classification of Disaster-Related Tweets Using Deep Learning\n",
        "\n",
        "## Introduction\n",
        "In this project, we will build a deep learning model using Keras to classify tweets as real or fake in the context of disasters. This task is inspired by the \"NLP with Disaster Tweets\" challenge and enriched with additional data to improve model performance and insights. The dataset provides a fascinating opportunity to explore Natural Language Processing (NLP) techniques on real-world data.\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset Overview\n",
        "### Context\n",
        "The dataset contains over 11,000 tweets associated with disaster-related keywords such as \"crash,\" \"quarantine,\" and \"bush fires.\" The data structure is based on the original \"Disasters on social media\" dataset. It includes:\n",
        "- **Tweets:** The text of the tweet.\n",
        "- **Keywords:** Specific disaster-related keywords.\n",
        "- **Location:** The geographical information provided in the tweets.\n",
        "\n",
        "These tweets were collected on **January 14th, 2020** and cover major events including:\n",
        "- The eruption of Taal Volcano in Batangas, Philippines.\n",
        "- The emerging outbreak of **Coronavirus (COVID-19)**.\n",
        "- The devastating **Bushfires in Australia**.\n",
        "- The **Iranian downing of flight PS752**.\n",
        "\n",
        "### Important Note\n",
        "The dataset contains text that may include profane, vulgar, or offensive language. Please approach with caution during analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## Project Goals\n",
        "### Inspiration\n",
        "The primary goal of this project is to develop a machine learning model capable of identifying whether a tweet is genuinely related to a disaster or not. This involves:\n",
        "1. Enriching the already available data with newly collected, manually classified tweets.\n",
        "2. Leveraging state-of-the-art deep learning methods to extract meaningful insights.\n",
        "3. Applying NLP techniques to preprocess, clean, and tokenize the tweets for model training.\n",
        "\n",
        "This notebook will walk through the process of preparing the dataset, building a deep learning model, and evaluating its performance. By the end, we aim to achieve a robust model that can classify disaster tweets with high accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## Why It Matters\n",
        "Effective classification of disaster-related tweets has numerous practical applications:\n",
        "- **Emergency Response:** Helps organizations identify critical information in real time.\n",
        "- **Resource Allocation:** Facilitates better planning by focusing on real disasters.\n",
        "- **Misinformation Control:** Mitigates the spread of false information during crises."
      ],
      "metadata": {
        "id": "9XshkOLmRDf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "HcOv7JePRGWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "qYHdNYEyRvS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('tweets.csv')\n",
        "\n",
        "# Display the first few rows to inspect the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Display dataset information (columns, data types, non-null counts)\n",
        "print(data.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpLaQCsQTbg1",
        "outputId": "cecf39ee-93ca-4e7c-83ab-7fd2128ce6f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id keyword        location  \\\n",
            "0   0  ablaze             NaN   \n",
            "1   1  ablaze             NaN   \n",
            "2   2  ablaze   New York City   \n",
            "3   3  ablaze  Morgantown, WV   \n",
            "4   4  ablaze             NaN   \n",
            "\n",
            "                                                text  target  \n",
            "0  Communal violence in Bhainsa, Telangana. \"Ston...       1  \n",
            "1  Telangana: Section 144 has been imposed in Bha...       1  \n",
            "2  Arsonist sets cars ablaze at dealership https:...       1  \n",
            "3  Arsonist sets cars ablaze at dealership https:...       1  \n",
            "4  \"Lord Jesus, your love brings freedom and pard...       0  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11370 entries, 0 to 11369\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        11370 non-null  int64 \n",
            " 1   keyword   11370 non-null  object\n",
            " 2   location  7952 non-null   object\n",
            " 3   text      11370 non-null  object\n",
            " 4   target    11370 non-null  int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 444.3+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure the dataset contains the required columns, such as:\n",
        "- `text`: The tweet content.\n",
        "- `label`: The classification label indicating whether the tweet is fake or not."
      ],
      "metadata": {
        "id": "dcPeZf5FTjFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify required columns\n",
        "assert 'text' in data.columns, \"Column 'text' is missing in the dataset.\"\n",
        "assert 'target' in data.columns, \"Column 'target' is missing in the dataset.\""
      ],
      "metadata": {
        "id": "yqjhyGBwThDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will split the dataset into training and validation sets using an 80%-20% ratio."
      ],
      "metadata": {
        "id": "7_fg7YBETl4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Features (tweet content) and labels (fake/true)\n",
        "X = data['text']       # Features\n",
        "y = data['target']      # Labels\n",
        "\n",
        "# Split the dataset (80% training, 20% validation)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "GeGrzSYBTnj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the training and validation datasets as separate CSV files for later use."
      ],
      "metadata": {
        "id": "8LNG0zrzTuHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine features and labels into dataframes\n",
        "train_df = pd.DataFrame({'text': X_train, 'label': y_train})\n",
        "test_df = pd.DataFrame({'text': X_test, 'label': y_test})\n",
        "\n",
        "# Save the dataframes to CSV files\n",
        "train_df.to_csv('train.csv', index=False)\n",
        "test_df.to_csv('test.csv', index=False)\n",
        "\n",
        "print(\"Datasets have been saved successfully:\")\n",
        "print(\"- Training set: train.csv\")\n",
        "print(\"- Validation set: test.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMnWSea3Tvv1",
        "outputId": "cd62abf0-9785-4674-eacc-0e39c3ebb811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets have been saved successfully:\n",
            "- Training set: train.csv\n",
            "- Validation set: test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Visualization\n",
        "\n",
        "In this section, we conduct a detailed exploratory data analysis (EDA) to understand the structure and distribution of our dataset. EDA is crucial for identifying potential challenges, trends, and biases within the data, which in turn helps in selecting the most suitable models and preprocessing steps.\n",
        "\n",
        "### 2.1 Dataset Size\n",
        "\n",
        "First, we print the size of both the training and testing datasets. This gives us an idea of how many data points we are working with, which is essential when evaluating model performance and understanding the balance of the dataset.\n"
      ],
      "metadata": {
        "id": "jKCAkGGcLs3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training dataset size: \", len(X_train))\n",
        "print(\"Testing dataset size: \", len(X_test))"
      ],
      "metadata": {
        "id": "UmgMh_5rMzU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Class Distribution in Training Data\n",
        "Next, we examine the distribution of the target variable in the training set. The target variable indicates whether a tweet is related to a **disaster (1) or not (0)**. Understanding the class distribution helps identify if the dataset is imbalanced, which could influence the choice of model or evaluation metrics (e.g., using precision-recall curves instead of accuracy)."
      ],
      "metadata": {
        "id": "KD4GX_WXM7N-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the count of tweets for each target class in the training set\n",
        "X_train['target'].value_counts()"
      ],
      "metadata": {
        "id": "66gGWmGzNmBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then plot a histogram of the target variable to visualize the distribution of disaster and non-disaster tweets in the training data. This graphical representation makes it easier to spot any imbalance."
      ],
      "metadata": {
        "id": "f52ANSCjNqGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train['target'].hist()\n",
        "plt.ylabel(\"# tweets\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-IhHDu7xNtWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Exploratory Analysis of Tweet Length\n",
        "### 2.3.1 Word Count per Tweet\n",
        "A useful aspect of text data is the length of the text. Here, we explore the number of words per tweet. This metric can help us understand the average tweet size for both disaster-related and non-disaster-related tweets, which may inform decisions on feature engineering, such as tokenization or padding.\n",
        "\n",
        "We use histograms to display the word count for each category of tweets (disaster vs. non-disaster)."
      ],
      "metadata": {
        "id": "enq8QFrDNxd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of words per tweet for each category (disaster vs non-disaster)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
        "\n",
        "tweet_len_0 = X_train[X_train['target'] == 0]['text'].str.split().map(lambda x: len(x))  # Non-disaster tweets\n",
        "tweet_len_1 = X_train[X_train['target'] == 1]['text'].str.split().map(lambda x: len(x))  # Disaster tweets\n",
        "\n",
        "ax1.hist(tweet_len_0, color='green')\n",
        "ax1.set_title('Non-disaster tweets')\n",
        "\n",
        "ax2.hist(tweet_len_1, color='red')\n",
        "ax2.set_title('Disaster tweets')\n",
        "\n",
        "fig.suptitle('Word Count per Tweet')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yIBnAGqyN9hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3.2 Unique Word Count per Tweet\n",
        "Next, we analyze the number of unique words per tweet. This measure indicates how diverse the vocabulary is for each tweet and can be an important factor when building feature sets **like word embeddings or TF-IDF.**"
      ],
      "metadata": {
        "id": "1dLpfGHaOI6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of unique words per tweet for each category\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
        "\n",
        "tweet_len_0 = X_train[X_train['target'] == 0]['text'].str.split().map(lambda x: len(set(x)))  # Non-disaster tweets\n",
        "tweet_len_1 = X_train[X_train['target'] == 1]['text'].str.split().map(lambda x: len(set(x)))  # Disaster tweets\n",
        "\n",
        "ax1.hist(tweet_len_0, color='green')\n",
        "ax1.set_title('Non-disaster tweets')\n",
        "\n",
        "ax2.hist(tweet_len_1, color='red')\n",
        "ax2.set_title('Disaster tweets')\n",
        "\n",
        "fig.suptitle('Unique Word Count per Tweet')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kP3fAQrLOkdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.3 Average Word Length per Tweet\n",
        "Lastly, we investigate the average length of the words used in the tweets. This measure provides insight into the complexity or simplicity of the language used in disaster vs. non-disaster tweets. A higher average word length might indicate more formal or technical language, whereas shorter words could suggest more informal communication."
      ],
      "metadata": {
        "id": "nOPwjjZnObNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average word length per tweet for each category\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
        "\n",
        "tweet_len_0 = X_train[X_train['target'] == 0]['text'].str.split().map(lambda x: np.mean([len(i) for i in x]))  # Non-disaster tweets\n",
        "tweet_len_1 = X_train[X_train['target'] == 1]['text'].str.split().map(lambda x: np.mean([len(i) for i in x]))  # Disaster tweets\n",
        "\n",
        "ax1.hist(tweet_len_0, color='green')\n",
        "ax1.set_title('Non-disaster tweets')\n",
        "\n",
        "ax2.hist(tweet_len_1, color='red')\n",
        "ax2.set_title('Disaster tweets')\n",
        "\n",
        "fig.suptitle('Average Word Length per Tweet')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uoO_vlmmOfJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Further Feature Calculations\n",
        "\n",
        "In addition to the basic tweet length analysis, we could calculate several other features that may provide additional insights for modeling. These features include:\n",
        "\n",
        "* Number of **words** at the end of a tweet\n",
        "* Number of **URLs** per tweet\n",
        "* **Average** number of characters per tweet\n",
        "* Number of **characters** per tweet\n",
        "* Number of punctuation marks per tweet\n",
        "* Number of **hashtags** per tweet\n",
        "* Number of **mentions** (@) per tweet\n",
        "\n",
        "These additional features could be crucial when constructing advanced models or for improving the understanding of tweet content."
      ],
      "metadata": {
        "id": "nZbot-lqMuW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Stopwords Analysis\n",
        "\n",
        "Stopwords are words that do not carry significant meaning by themselves, but help structure or modify other words in a sentence. These include articles, pronouns, prepositions, adverbs, and some verbs. In natural language processing (NLP), stopwords are typically removed because they do not add value to the analysis. For example, search engines like Google do not consider stopwords when indexing content, but they are used when displaying results.\n",
        "\n",
        "To explore which stopwords are most common in the dataset, we can use the following approach:\n",
        "\n"
      ],
      "metadata": {
        "id": "OMf3sLL0P4B9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords.words('english')\n",
        "\n",
        "def plot_stopwords(label):\n",
        "    tweets_stopwords = {}\n",
        "    for words in X_train[X_train['target'] == label]['text'].str.split():\n",
        "        sw = list(set(words).intersection(stopwords.words('english')))\n",
        "        for w in sw:\n",
        "            if w in tweets_stopwords.keys():\n",
        "                tweets_stopwords[w] += 1\n",
        "            else:\n",
        "                tweets_stopwords[w] = 1\n",
        "\n",
        "    top = sorted(tweets_stopwords.items(), key=lambda x:x[1],reverse=True)[:10]\n",
        "    plt.bar(*zip(*top))\n",
        "    plt.show()\n",
        "\n",
        "plot_stopwords(0)\n",
        "plot_stopwords(1)"
      ],
      "metadata": {
        "id": "qSu_lhXcQKqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will display the 10 most frequent stopwords in disaster and non-disaster tweets separately."
      ],
      "metadata": {
        "id": "vU1BvrsbQNDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Punctuation Marks Analysis\n",
        "\n",
        "Next, we analyze the punctuation marks used in the tweets. Punctuation marks can play a role in sentiment analysis and text classification. We examine the most frequently used punctuation marks in both disaster-related and non-disaster-related tweets:"
      ],
      "metadata": {
        "id": "bcYwaomVQ_dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def plot_punctuation(label):\n",
        "    tweets_punctuation = {}\n",
        "    for words in X_train[X_train['target'] == label]['text'].str.split():\n",
        "        sw = list(set(words).intersection(string.punctuation))\n",
        "        for w in sw:\n",
        "            if w in tweets_punctuation.keys():\n",
        "                tweets_punctuation[w] += 1\n",
        "            else:\n",
        "                tweets_punctuation[w] = 1\n",
        "\n",
        "    top = sorted(tweets_punctuation.items(), key=lambda x:x[1], reverse=True)[:20]\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(*zip(*top))\n",
        "    plt.show()\n",
        "\n",
        "plot_punctuation(0)\n",
        "plot_punctuation(1)\n"
      ],
      "metadata": {
        "id": "peCtQivpRGC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will plot the most common punctuation marks for both types of tweets."
      ],
      "metadata": {
        "id": "cu06gHWIRKAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 N-grams Analysis\n",
        "\n",
        "Finally, we perform an analysis of n-grams, which are sequences of n consecutive words. N-grams are useful for capturing patterns in text and can help us understand frequently occurring phrases or expressions."
      ],
      "metadata": {
        "id": "J6cTTlJQRMvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(ngram_range=(2, 2))\n",
        "sum_words = cv.fit_transform(X_train['text']).sum(axis=0)\n",
        "\n",
        "# Calculate frequency of n-grams\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)[:20]\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.barh(*zip(*words_freq))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7fXycAI9RRYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This analysis will highlight the top 20 most frequent 2-grams (bigrams) in the dataset."
      ],
      "metadata": {
        "id": "exmLrA7LRdjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These visualizations and additional feature calculations provide deeper insights into the structure and content of the dataset. By understanding features like word count, stopwords, punctuation marks, and n-grams, we can better prepare our data for modeling. Moreover, the analysis helps identify any potential biases or patterns that could influence the performance of the model, ensuring that we approach the task of disaster tweet classification in a well-informed manner."
      ],
      "metadata": {
        "id": "2X5j6EoGRflu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Cleaning\n",
        "\n",
        "Data cleaning is a crucial step in preparing a dataset for analysis and machine learning. The goal is to remove unnecessary or problematic elements from the data that could potentially introduce noise or lead to incorrect interpretations. In this section, we will apply several techniques to clean the tweet texts in our dataset.\n",
        "\n",
        "### 3.1 Removing URLs\n",
        "\n",
        "One common issue in text data, especially from social media platforms like Twitter, is the presence of URLs. URLs can distract the model and are often irrelevant to the task of tweet classification. Therefore, we define a function to remove URLs from the text using regular expressions.\n"
      ],
      "metadata": {
        "id": "BdUKRd7lSpY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_url(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'', text)\n",
        "\n",
        "# Example of removing a URL from a tweet with a different structure\n",
        "remove_url(\"Check out this amazing article on machine learning: https://www.example.com/articles/deep-learning-tutorial\")"
      ],
      "metadata": {
        "id": "gSY6bhoJStNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Removing HTML Tags\n",
        "HTML tags may appear in some tweets, especially when they contain rich text or references to web pages. These tags don't contribute to the meaning of the tweet and should be removed. We can use the `HTMLParser` class to strip HTML tags from the text."
      ],
      "metadata": {
        "id": "kB5WLPuRSxUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from html.parser import HTMLParser\n",
        "\n",
        "class HTMLStripper(HTMLParser):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self.strict = False\n",
        "        self.convert_charrefs = True\n",
        "        self.fed = []\n",
        "\n",
        "    def handle_data(self, d):\n",
        "        self.fed.append(d)\n",
        "\n",
        "    def get_data(self):\n",
        "        return ''.join(self.fed)\n",
        "\n",
        "def remove_html(text):\n",
        "    s = HTMLStripper()\n",
        "    s.feed(text)\n",
        "    return s.get_data()\n",
        "\n",
        "# Example of removing HTML tags from a string\n",
        "remove_html('<tr><td align=\"left\"><a href=\"../../issues/51/16.html#article\">Phrack World News</a></td>')"
      ],
      "metadata": {
        "id": "240GoYrAS45U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Removing Emojis\n",
        "\n",
        "Emojis are often used in tweets to convey emotions or sentiments. While they can be useful for sentiment analysis, they are generally not relevant for a classification task such as disaster prediction. We remove emojis using a regular expression pattern that targets Unicode characters corresponding to emojis."
      ],
      "metadata": {
        "id": "aPPxffr6S8oP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "# Example of removing emojis from a tweet\n",
        "remove_emoji(\"Omg another Earthquake 😔😔\")"
      ],
      "metadata": {
        "id": "EldhJXg9TCPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Removing Punctuation\n",
        "Punctuation marks can sometimes be useful in text analysis, but they often do not add meaningful information for classification tasks. For simplicity, we can remove punctuation from the tweets."
      ],
      "metadata": {
        "id": "C78smgqaTHxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Example of removing punctuation from a tweet\n",
        "remove_punctuation(\"hello #how are you?\")"
      ],
      "metadata": {
        "id": "xGxvDgaMTNUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Applying Cleaning Functions to the Dataset\n",
        "Now that we have defined the functions to remove** URLs, HTML tags, emojis**, and punctuation, we apply them to both the training and testing datasets. This ensures that the data is cleaned before any further processing or model training."
      ],
      "metadata": {
        "id": "6bgI_8vRTPrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply cleaning functions to the training set\n",
        "X_train_prep = X_train.copy()\n",
        "X_train_prep['text'] = X_train['text'].apply(remove_url)\n",
        "X_train_prep['text'] = X_train['text'].apply(remove_html)\n",
        "X_train_prep['text'] = X_train['text'].apply(remove_emoji)\n",
        "X_train_prep['text'] = X_train['text'].apply(remove_punctuation)\n",
        "\n",
        "# Apply cleaning functions to the testing set\n",
        "X_test_prep = X_test.copy()\n",
        "X_test_prep['text'] = X_test['text'].apply(remove_url)\n",
        "X_test_prep['text'] = X_test['text'].apply(remove_html)\n",
        "X_test_prep['text'] = X_test['text'].apply(remove_emoji)\n",
        "X_test_prep['text'] = X_test['text'].apply(remove_punctuation)"
      ],
      "metadata": {
        "id": "afQrjpBUTSfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 Analyzing N-Grams After Data Cleaning\n",
        "Once the data has been cleaned, we can proceed with feature extraction. For example, we can analyze n-grams (sequences of words) to see which combinations of words are most frequent in the cleaned training set. Here, we calculate the most frequent bigrams (two-word sequences)."
      ],
      "metadata": {
        "id": "VrOX51GRTYY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(ngram_range=(2, 2))\n",
        "sum_words = cv.fit_transform(X_train_prep['text']).sum(axis=0)\n",
        "\n",
        "# Calculate frequency of n-grams\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)[:20]\n",
        "\n",
        "# Plot the most frequent bigrams\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.barh(*zip(*words_freq))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mUabKLruTdhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data cleaning is a fundamental step in the text preprocessing pipeline. By removing irrelevant elements such as URLs, HTML tags, emojis, and punctuation marks, we ensure that our model focuses on the meaningful content of the tweets. Once the data is cleaned, we can proceed with further analysis and feature extraction, which will provide the model with cleaner, more relevant input for training."
      ],
      "metadata": {
        "id": "2qT2isGeTh9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Vectorization of the Dataset\n",
        "\n",
        "In this section, we will transform the textual data into numerical vectors using the **TF-IDF** (Term Frequency-Inverse Document Frequency) method. This is a common technique used in natural language processing (NLP) to convert text data into a form that can be fed into machine learning models. The goal of TF-IDF is to reflect the importance of a word within a document relative to its frequency across all documents.\n",
        "\n",
        "### 4.1 Preparing Target Labels\n",
        "\n",
        "Before performing the vectorization, we need to separate the target labels (the classification labels) from the input data (the text of the tweets). The target labels are stored in the `Y_train` variable, which contains the classification for each tweet (0 for non-disaster, 1 for disaster)."
      ],
      "metadata": {
        "id": "pGNcpAegU9ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train = X_train_prep['target']"
      ],
      "metadata": {
        "id": "ylZI30r_VFZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Applying TF-IDF Vectorization\n",
        "\n",
        "We use the **TfidfVectorizer** from **scikit-learn** to convert the tweet text into numerical vectors. This vectorizer transforms each tweet into a vector where each element represents the importance of a specific word in the tweet."
      ],
      "metadata": {
        "id": "OZz7fSzEVIDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train = vectorizer.fit_transform(X_train_prep['text'])\n",
        "\n",
        "# Convert the sparse matrix to a dense array for easier handling\n",
        "X_train = X_train.toarray()\n",
        "\n",
        "# Display the transformed training data\n",
        "X_train"
      ],
      "metadata": {
        "id": "IgwbgyHfVQLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the training data is vectorized, we apply the same transformation to the test set, which ensures that both training and testing data are represented in the same vector space."
      ],
      "metadata": {
        "id": "AYZVde3_VXh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the test data using the fitted vectorizer\n",
        "X_test = vectorizer.transform(X_test_prep['text'])\n",
        "\n",
        "# Convert the sparse matrix to a dense array\n",
        "X_test = X_test.toarray()"
      ],
      "metadata": {
        "id": "pr5cKIDKVarU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, both `X_train` and `X_test` contain the vectorized representations of the tweets, which can be used for training machine learning models."
      ],
      "metadata": {
        "id": "tchEu5ssVeBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Data Splitting\n",
        "In machine learning, it is essential to evaluate the model's performance on data that it hasn't seen during training. To do this, we split the data into training, validation, and testing sets.\n",
        "\n",
        "## 5.1 Splitting the Data into Training and Validation Sets\n",
        "We will first split the dataset into training and validation sets. This is done to evaluate the model's performance on data that it hasn't been trained on but still comes from the same distribution.\n",
        "\n",
        "We use the train_test_split function from scikit-learn to perform this split, reserving 15% of the data for validation."
      ],
      "metadata": {
        "id": "ZyDsAHKOVhyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and validation sets (15% for validation)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.15)\n",
        "\n",
        "# Print the sizes of the resulting datasets\n",
        "print(\"Training subset length: \", len(X_train))\n",
        "print(\"Validation subset length: \", len(X_val))\n",
        "print(\"Test subset length: \", len(X_test))"
      ],
      "metadata": {
        "id": "5Po6QFjKVmx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Dataset Summary\n",
        "After splitting the data, we print the lengths of the training, validation, and testing subsets to ensure that the split was performed correctly. The training data will be used to train the model, the validation data will be used for hyperparameter tuning and performance evaluation during training, and the test data will be used to evaluate the final model's performance."
      ],
      "metadata": {
        "id": "2W1bIM2MVqUx"
      }
    }
  ]
}