{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Classification of Disaster-Related Tweets Using Deep Learning\n",
        "\n",
        "## Introduction\n",
        "In this project, we will build a deep learning model using Keras to classify tweets as real or fake in the context of disasters. This task is inspired by the \"NLP with Disaster Tweets\" challenge and enriched with additional data to improve model performance and insights. The dataset provides a fascinating opportunity to explore Natural Language Processing (NLP) techniques on real-world data.\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset Overview\n",
        "### Context\n",
        "The dataset contains over 11,000 tweets associated with disaster-related keywords such as \"crash,\" \"quarantine,\" and \"bush fires.\" The data structure is based on the original \"Disasters on social media\" dataset. It includes:\n",
        "- **Tweets:** The text of the tweet.\n",
        "- **Keywords:** Specific disaster-related keywords.\n",
        "- **Location:** The geographical information provided in the tweets.\n",
        "\n",
        "These tweets were collected on **January 14th, 2020** and cover major events including:\n",
        "- The eruption of Taal Volcano in Batangas, Philippines.\n",
        "- The emerging outbreak of **Coronavirus (COVID-19)**.\n",
        "- The devastating **Bushfires in Australia**.\n",
        "- The **Iranian downing of flight PS752**.\n",
        "\n",
        "### Important Note\n",
        "The dataset contains text that may include profane, vulgar, or offensive language. Please approach with caution during analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## Project Goals\n",
        "### Inspiration\n",
        "The primary goal of this project is to develop a machine learning model capable of identifying whether a tweet is genuinely related to a disaster or not. This involves:\n",
        "1. Enriching the already available data with newly collected, manually classified tweets.\n",
        "2. Leveraging state-of-the-art deep learning methods to extract meaningful insights.\n",
        "3. Applying NLP techniques to preprocess, clean, and tokenize the tweets for model training.\n",
        "\n",
        "This notebook will walk through the process of preparing the dataset, building a deep learning model, and evaluating its performance. By the end, we aim to achieve a robust model that can classify disaster tweets with high accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## Why It Matters\n",
        "Effective classification of disaster-related tweets has numerous practical applications:\n",
        "- **Emergency Response:** Helps organizations identify critical information in real time.\n",
        "- **Resource Allocation:** Facilitates better planning by focusing on real disasters.\n",
        "- **Misinformation Control:** Mitigates the spread of false information during crises."
      ],
      "metadata": {
        "id": "9XshkOLmRDf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "HcOv7JePRGWU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "qYHdNYEyRvS9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('tweets.csv')\n",
        "\n",
        "# Display the first few rows to inspect the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Display dataset information (columns, data types, non-null counts)\n",
        "print(data.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpLaQCsQTbg1",
        "outputId": "cecf39ee-93ca-4e7c-83ab-7fd2128ce6f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id keyword        location  \\\n",
            "0   0  ablaze             NaN   \n",
            "1   1  ablaze             NaN   \n",
            "2   2  ablaze   New York City   \n",
            "3   3  ablaze  Morgantown, WV   \n",
            "4   4  ablaze             NaN   \n",
            "\n",
            "                                                text  target  \n",
            "0  Communal violence in Bhainsa, Telangana. \"Ston...       1  \n",
            "1  Telangana: Section 144 has been imposed in Bha...       1  \n",
            "2  Arsonist sets cars ablaze at dealership https:...       1  \n",
            "3  Arsonist sets cars ablaze at dealership https:...       1  \n",
            "4  \"Lord Jesus, your love brings freedom and pard...       0  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11370 entries, 0 to 11369\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        11370 non-null  int64 \n",
            " 1   keyword   11370 non-null  object\n",
            " 2   location  7952 non-null   object\n",
            " 3   text      11370 non-null  object\n",
            " 4   target    11370 non-null  int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 444.3+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure the dataset contains the required columns, such as:\n",
        "- `text`: The tweet content.\n",
        "- `label`: The classification label indicating whether the tweet is fake or not."
      ],
      "metadata": {
        "id": "dcPeZf5FTjFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify required columns\n",
        "assert 'text' in data.columns, \"Column 'text' is missing in the dataset.\"\n",
        "assert 'target' in data.columns, \"Column 'target' is missing in the dataset.\""
      ],
      "metadata": {
        "id": "yqjhyGBwThDU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will split the dataset into training and validation sets using an 80%-20% ratio."
      ],
      "metadata": {
        "id": "7_fg7YBETl4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Features (tweet content) and labels (fake/true)\n",
        "X = data['text']       # Features\n",
        "y = data['target']      # Labels\n",
        "\n",
        "# Split the dataset (80% training, 20% validation)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "GeGrzSYBTnj1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the training and validation datasets as separate CSV files for later use."
      ],
      "metadata": {
        "id": "8LNG0zrzTuHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine features and labels into dataframes\n",
        "train_df = pd.DataFrame({'text': X_train, 'label': y_train})\n",
        "test_df = pd.DataFrame({'text': X_test, 'label': y_test})\n",
        "\n",
        "# Save the dataframes to CSV files\n",
        "train_df.to_csv('train.csv', index=False)\n",
        "test_df.to_csv('test.csv', index=False)\n",
        "\n",
        "print(\"Datasets have been saved successfully:\")\n",
        "print(\"- Training set: train.csv\")\n",
        "print(\"- Validation set: test.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMnWSea3Tvv1",
        "outputId": "cd62abf0-9785-4674-eacc-0e39c3ebb811"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets have been saved successfully:\n",
            "- Training set: train.csv\n",
            "- Validation set: test.csv\n"
          ]
        }
      ]
    }
  ]
}